## Script that will parse the information on the Posts.xml file
## then extract the information and store the normalized data into HDFS

## To run this script, you must execute this command in terminal
## spark-submit  --packages com.databricks:spark-xml_2.10:0.3.3  --master local[*] post-to-hdfs.py file:///home/cloudera/ps/cdh-samples/stackoverflow-python-cdh/exercise-4/Posts.xml

from pyspark import SparkContext, SparkConf
import xml.etree.ElementTree as ET
import re
import sys

## Post types ids for elements contained inside the Posts.xml file
postTypes = {'1': 'Question', '2': 'Answer', '3': 'Wiki', '4': 'TagWikiExcerpt', '5': 'TagWiki', '6': 'ModeratorNomination', '7': 'WikiPlaceholder', '8': 'PrivilegeWiki'}

## Function used to process a row entry, and get the necessary information from it
def processRows(xml):
	elements = ET.fromstring(xml.encode('utf-8')).attrib
	## Tags need to be cleaned a bit. We only need the words, avoid the <> characters.
	tags = re.findall(r'[\w.-]+', elements.get('Tags', ''))
	body = elements.get('Body')
	title = elements.get('Title')
	postId = elements.get('Id')
	postType = postTypes[elements.get('PostTypeId')]
	answerCount = elements.get('AnswerCount')
	commentCount = elements.get('CommentCount')
	creationDate = elements.get('CreationDate')
	score = elements.get('Score')
	lastActivityDate = elements.get('LastActivityDate')
	acceptedAnswerId = elements.get('AcceptedAnswerId')
	ownerUserId = elements.get('OwnerUsedId')
	lastEditorUserId = elements.get('LastEditorUserId')
	return (postId, postType, title, body, tags, answerCount, commentCount, creationDate, score, lastActivityDate, acceptedAnswerId, ownerUserId, lastEditorUserId)

if __name__ == "__main__":
	## Initialize Spark Context
	conf = SparkConf().setAppName("XMLPARSER").setMaster("local")
	sc = SparkContext(conf=conf)

	## File to read
	postsFile = sys.argv[1] if len(sys.argv) > 1 else 'Posts.xml'

	## Directory to output the processed RDD
	output = sys.argv[2] if len(sys.argv) > 2 else 'results'

	## Provide start and end tags for xml entrees
	xmlConf = {'xmlinput.start': '<row', 'xmlinput.end': '/>'}

	## Read file using hadoop using the spark-xml input format
	records = sc.newAPIHadoopFile(postsFile, 'com.databricks.spark.xml.XmlInputFormat', 'org.apache.hadoop.io.Text', 'org.apache.hadoop.io.Text', conf=xmlConf)

	## Remove autogenerated ids
	normalizedRecords = records.map(lambda x: x[1])

	## Extract the required fields
	rdd = normalizedRecords.map(processRows)

	## Save the generated RDD into HDFS
	rdd.saveAsPickleFile(output)

	sc.stop()
